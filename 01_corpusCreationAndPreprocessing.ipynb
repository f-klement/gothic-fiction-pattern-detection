{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus Creation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gutenbergpy.gutenbergcache import GutenbergCache, GutenbergCacheSettings, GutenbergCacheTypes\n",
    "#import gutenbergpy.textget \n",
    "from gensim.models import Word2Vec\n",
    "from joblib import dump\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and cache the Project Gutenberg Metadata to query it\n",
    "#GutenbergCache.create() #refresh=False, download=True, unpack=True, parse=True, cache=True, deleteTemp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The titles for the corpus are taken from several sources: While the list of books collected by Caroline Winter and Eleanor Stribling as part of their works on exploring the color space in use in texts of gothic fiction, contributed both names and texts themselves, all further texts have been taken from Project Gutenberg. Ted Underwood created a dataframe of metadata on texs that David Punters and Glennis Byrons regard as the most prominent contributors to the genre within their seminal works on the Gothic. Many of the titles on that list have been gathered and enriched with further titles form authors mentioned in their text 'The Gothic' as well as the titles that Project Gutenberg attributes to this or closely related genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache = GutenbergCache.get_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor=cache.native_query(\"\"\"\n",
    "#                           SELECT \n",
    "#                            MAX(b.gutenbergbookid) as book_id,\n",
    "#                            CASE \n",
    "#                                 WHEN MAX(a.name) = 'Boz' THEN 'Dickens, Charles'\n",
    "#                                 WHEN MAX(a.name) = 'Marshall, William' THEN 'Walpole, Horace'\n",
    "#                                 WHEN MAX(a.name) = 'Grile, Dod' THEN 'Bierce, Ambrose'\n",
    "#                                 ELSE MAX(a.name) \n",
    "#                             END as authors,\n",
    "#                             t.name as titles\n",
    "#                           FROM books b \n",
    "#                           join languages l on l.id=b.languageid and l.name='en'\n",
    "#                           join book_subjects bsu on b.id=bsu.bookid\n",
    "#                           join bookshelves bsh on b.bookshelveid=bsh.id\n",
    "#                           join subjects s on s.id=bsu.subjectid\n",
    "#                           join titles t on t.bookid=b.id\n",
    "#                           join book_authors ba on ba.bookid=b.id\n",
    "#                           join authors a on ba.authorid=a.id\n",
    "#                           where \n",
    "#                                 (lower(bsh.name) like '%horror%'\n",
    "#                                 or lower(s.name) like '%horror%'\n",
    "#                                 or lower(bsh.name) like '%gothic%'\n",
    "#                                 or lower(s.name) like '%gothic%'\n",
    "#                                 or lower(bsh.name) like '%supernatural%'\n",
    "#                                 or lower(s.name) like '%supernatural%'\n",
    "#                                 or lower(bsh.name) like '%paranormal%'\n",
    "#                                 or lower(s.name) like '%paranormal%'\n",
    "#                                 or lower(bsh.name) like '%vampire%'\n",
    "#                                 or lower(s.name) like '%vampire%'\n",
    "#                                 or lower(bsh.name) like '%ghost%'\n",
    "#                                 or lower(s.name) like '%ghost%')\n",
    "#                                 and a.name not in \n",
    "#                                 ('Arthur, Robert','Baker, Frank','Baldwin, Edward','Birkhead, Edith',\n",
    "#                                 'Blackwood, Algernon','Bloxam, Matthew Holbeche','De Quincey, Thomas',\n",
    "#                                 'DeQuincey, Thomas',\n",
    "#                                 'De Vet, Charles V.','Glad, Victoria','Hammond, Keith', 'Hodgson, William Hope',\n",
    "#                                 'Hopkins, R. Thurston (Robert Thurston)','Kafka, F. (Franz)','La Spina, Greye',\n",
    "#                                 'Leroux, Gaston','Littlewit, Humphrey','Marks, Winston K. (Winston Kinney)',\n",
    "#                                 'O''Donnell, Elliot', 'Oliver, George', 'Olivieri, David', 'Kafka, Franz',\n",
    "#                                 'Peterson, Don', 'Lovecraft, H. P. (Howard Phillips)','Phillips, Forbes',\n",
    "#                                 'Tenneshaw, S. M.','Weinbaum, Stanley G. (Stanley Grauman)')\n",
    "#                                 and b.id not in (12728, 12739, 12751, 12762) -- collected works\n",
    "#                                 and b.gutenbergbookid not in (2147, 31469, 50133, 50133,2040, 42324, 41445, 9629,\n",
    "#                                 50133, 24022, 20673, 20038, 13334, 6542, 19505, 19337, 18233,20034,14317,\n",
    "#                                 14168,32076,18233,20034,14317, 24350, 25037) -- ill-fitting titles\n",
    "#                             GROUP BY t.name\n",
    "#                           \"\"\")\n",
    "\n",
    "# results = cursor.fetchall()\n",
    "# df = pd.DataFrame(results, columns=[column[0] for column in cursor.description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the package does not fetch biographical data on the authors or original publication dates, the filtering process had to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function downloads a book by its Gutenberg ID\n",
    "# def download_book(book_id):\n",
    "#     raw_book = gutenbergpy.textget.get_text_by_id(book_id)  # with headers\n",
    "#     clean_book = gutenbergpy.textget.strip_headers(raw_book)  # without headers\n",
    "#     return clean_book\n",
    "\n",
    "# df['text'] = ''\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     try:\n",
    "#         df.loc[idx, 'text'] = download_book(row['book_id'])\n",
    "#     except Exception:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./preprocessing/corpora/Gutenberg_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of names obtained from the Table of contents of David Punters and Glennis Byrons book the Gothic filtered for all authors that were active before the beginning of the 20th centuary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names= ['William Harrison Ainsworth','William Beckford','E. F. Benson','Ambrose Bierce','Elizabeth Bowen','Mary Elizabeth Braddon',\n",
    "# 'Charles Brockden Brown','Robert W. Chambers','Wilkie Collins','Marie Corelli','Charlotte Dacre','Walter de la Mare',\n",
    "# 'Isak Dinesen','Elizabeth Gaskell','William Godwin','H. Rider Haggard','Nathaniel Hawthorne', 'William Hope Hodgson',\n",
    "# 'James Hogg','Washington Irving','G. P. R. James','Henry James','Francis Lathom','J. Sheridan Le Fanu','Sophia Lee','Vernon Lee','Matthew Lewis',\n",
    "# 'Bulwer Lytton','George MacDonald','Arthur Machen', 'James Macpherson','Charles Robert Maturin','Herman Melville','Margaret Oliphant','Edgar Allan Poe',\n",
    "# 'John Polidori','Ann Radcliffe','Clara Reeve','G. W. M. Reynolds','Walter Scott','Mary Wollstonecraft Shelley','Charlotte Smith','Tobias Smollett',\n",
    "# 'Robert Louis Stevenson','Bram Stoker','Horace Walpole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_names = []\n",
    "# for name in names:\n",
    "#     name_parts = name.split(\" \")\n",
    "#     last_name = name_parts[-1]\n",
    "#     other_names = name_parts[:-1]\n",
    "#     formatted_name = last_name + ', ' + \" \".join(other_names)\n",
    "#     formatted_names.append(formatted_name)\n",
    "\n",
    "# names_string = ', '.join(f\"'{name}'\" for name in formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor=cache.native_query(f\"\"\"\n",
    "#                           SELECT \n",
    "#                             MAX(b.gutenbergbookid) as book_id,\n",
    "#                             MAX(a.name) as authors,\n",
    "#                             t.name as titles\n",
    "#                           FROM books b \n",
    "#                           join languages l on l.id=b.languageid and l.name='en'\n",
    "#                           join book_subjects bsu on b.id=bsu.bookid\n",
    "#                           join bookshelves bsh on b.bookshelveid=bsh.id\n",
    "#                           join subjects s on s.id=bsu.subjectid\n",
    "#                           join titles t on t.bookid=b.id\n",
    "#                           join book_authors ba on ba.bookid=b.id\n",
    "#                           join authors a on ba.authorid=a.id\n",
    "#                           where \n",
    "#                           (a.name in ({names_string}) or t.name in ('Wuthering Heights', 'Hieroglyphic Tales', 'Dreams, Waking Thoughts, and Incidents','Vathek; An Arabian Tale',\n",
    "#                           'Melmoth the Wanderer, Vol. 1','Melmoth the Wanderer, Vol. 2','Melmoth the Wanderer, Vol. 3','A Sicilian Romance','The Evil Guest',\n",
    "#                           'Melmoth the Wanderer, Vol. 4','Auriol; or, The Elixir of Life', 'The Children of the Abbey: A Tale', 'The Dead Secret: A Novel',\n",
    "#                           'The Wyvern mystery', 'A Stable for Nightmares; or, Weird Tales', 'The House by the Church-Yard', 'Green Tea; Mr. Justice Harbottle',\n",
    "#                           'After Dark', 'The Haunted Hotel: A Mystery of Modern Venice', 'Armadale', 'The Queen of Hearts', \n",
    "#                           'The Frozen Deep', 'The Dead Secret: A Novel', 'The Legacy of Cain', 'The Black Robe','Heart and Science: A Story of the Present Time',\n",
    "#                           'Phantom Fortune, a Novel', 'The Little Red Foot', 'In Search of the Unknown', 'The Moonlit Way: A Novel', 'The Slayer of Souls',\n",
    "#                           'The Hidden Children', 'Phantastes: A Faerie Romance for Men and Women', 'The Old English Baron: a Gothic Story', 'Wagner, the Wehr-Wolf',\n",
    "#                           'Emmeline, the Orphan of the Castle', 'The House on the Borderland', 'Carnacki, the Ghost Finder', 'A Phantom Lover', 'The People of the Mist',\n",
    "#                           'The Sorrows of Satan', 'The soul of Lilith'))\n",
    "#                           and a.name not in ('Mare, Walter de la', 'Melville, Herman','Arthur, Robert','Baker, Frank',\n",
    "#                           'Baldwin, Edward','Birkhead, Edith','Blackwood, Algernon','Bloxam, Matthew Holbeche',\n",
    "#                           'De Quincey, Thomas','DeQuincey, Thomas','De Vet, Charles V.','Glad, Victoria',\n",
    "#                           'Hammond, Keith', 'Hodgson, William Hope','Mare, Walter de la',\n",
    "#                           'Hopkins, R. Thurston (Robert Thurston)','Kafka, F. (Franz)','La Spina, Greye',\n",
    "#                           'Leroux, Gaston','Littlewit, Humphrey','Marks, Winston K. (Winston Kinney)',\n",
    "#                           'O''Donnell, Elliot', 'Oliver, George', 'Olivieri, David', 'Kafka, Franz',\n",
    "#                           'Peterson, Don', 'Lovecraft, H. P. (Howard Phillips)','Phillips, Forbes',\n",
    "#                           'Tenneshaw, S. M.','Weinbaum, Stanley G. (Stanley Grauman)')\n",
    "#                           and b.id not in (12728, 12739, 12751, 12762) \n",
    "#                           and b.gutenbergbookid not in (2147,8939, 8940,7023,7024, 31469, 50133, 50133,2040, 42324, 41445, 9629,\n",
    "#                           50133, 24022, 20673, 20038, 13334, 6542,4964, 4965,4966, 864,25611,14535,19505, 19337, 18233,20034,14317,12396,12397,120,\n",
    "#                           14168,32076,18233,20034,14317, 24350, 25037, 13334, 14082, 6942, 6943,2590, 5355, 5354, 5353, 6661,616,26458,\n",
    "#                           13334,9377, 771, 30486, 3606, 15697, 20656, 2834, 2833, 12031, 12032,23545, 12031, 589, 382, 421,441,322, 25617,848)\n",
    "#                           and t.name not in ('The Star-Chamber: An Historical Romance, Volume 2', 'The Star-Chamber: An Historical Romance, Volume 1',\n",
    "#                           'True Stories of History and Biography','The Scarlet Letter','Twice-told tales', 'Old Christmas: from the Sketch Book of Washington Irving',\n",
    "#                           'Astoria; Or, Anecdotes of an Enterprise Beyond the Rocky Mountains','The Portrait of a Lady — Volume 1',\n",
    "#                           'The Portrait of a Lady — Volume 1', 'The Light Princess and Other Fairy Stories')\n",
    "#                           GROUP BY t.name\n",
    "#                           ORDER BY a.name, t.name\n",
    "#                           \"\"\")\n",
    "\n",
    "# results = cursor.fetchall()\n",
    "# df = pd.DataFrame(results, columns=[column[0] for column in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function downloads a book by its Gutenberg ID\n",
    "# def download_book(book_id):\n",
    "#     raw_book = gutenbergpy.textget.get_text_by_id(book_id)  # with headers\n",
    "#     clean_book = gutenbergpy.textget.strip_headers(raw_book)  # without headers\n",
    "#     return clean_book\n",
    "\n",
    "# df['text'] = ''\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     try:\n",
    "#         df.loc[idx, 'text'] = download_book(row['book_id'])\n",
    "#     except Exception:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_init=pd.read_csv('./preprocessing/corpora/Gutenberg_texts.csv')\n",
    "# df['source'] = 'liste'\n",
    "# df_init['source'] = 'bookshelf'\n",
    "# union_df = pd.concat([df, df_init])\n",
    "# sorted_df = union_df.sort_values('authors')\n",
    "# unique_df = sorted_df.drop_duplicates(subset='titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_df.to_csv('./preprocessing/corpora/Gutenberg_texts_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gutenberg=pd.read_csv('./preprocessing/corpora/Gutenberg_texts_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the limitet amount that the python package is capable of retrieving, manual downloads will be necessary as well. Given the quality of the texts and the excessive paratexts, some cleaning will be necessary as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gothic colors corpus contains a number of intersting texts, as well as highly relevant metadata, we shall make us of here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shall be restricted to relevant texts, joined with our existing dataframe and the files read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors=pd.read_csv('./preprocessing/corpora/gothic_texts.csv')\n",
    "# df_colors = df_colors[~(df_colors['Nationality'].isin(['French', 'German', 'Italian', 'Sicilian', 'Swiss']) | \n",
    "#            df_colors['Genre'].isin(['Aesthetic Theory','Criticism', 'Literary Theory', 'Memoir/Biography', 'Review', 'Travel Writing']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors['Text'] = \"\"\n",
    "# dir_path = \"/Storage/Studium/DigitalHumanities/Semester5/Thesis/code_notebooks/color_corpus/corpora/\"\n",
    "\n",
    "\n",
    "# for index, row in df_colors.iterrows():\n",
    "#     if pd.notnull(row['Filename']):  \n",
    "#         file_path = os.path.join(dir_path, row['Filename'])\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             text = file.read()  \n",
    "#         df_colors.at[index, 'Text'] = text \n",
    "\n",
    "\n",
    "# filled_rows = df_colors[df_colors['Text'] != ''].shape[0]\n",
    "# print(filled_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are normalization to homogenize the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors.columns = df_colors.columns.str.lower()\n",
    "# df_colors['source'] = 'colors corpus'\n",
    "# df_gutenberg = df_gutenberg.rename(columns={'authors': 'author','titles': 'title'})\n",
    "# df_combined = pd.concat([df_colors, df_gutenberg])\n",
    "# df_combined['text'].replace('', np.nan, inplace=True)\n",
    "# df_combined['text_filled'] = df_combined['text'].notna().astype(int)\n",
    "# df_combined[\"sort_helper\"] = df_combined[\"source\"].apply(lambda x: 0 if x == \"colors corpus\" else 1)\n",
    "# df_combined = df_combined.sort_values(by=['text_filled', 'author', 'title', 'sort_helper'], ascending=[False, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Export of the first five columns for manual correction before unification\n",
    "#df_combined.iloc[:, :5].to_csv('./preprocessing/intermediary_steps/Combined_texts_non_tidy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the data back into a DataFrame\n",
    "# df_imported = pd.read_csv('./preprocessing/intermediary_steps/Combined_texts_non_tidy.csv')\n",
    "# df_combined[df_imported.columns] = df_imported\n",
    "# df_combined = df_combined.sort_values(by=['text_filled', 'author', 'title', 'sort_helper'], ascending=[False, True, True, True])\n",
    "# df_combined = df_combined.drop_duplicates(subset=['author', 'title'], keep='first')\n",
    "# df_combined.drop(['text_filled', 'sort_helper'], axis=1, inplace=True)\n",
    "# df_combined = df_combined.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined.drop(['publisher', 'pseudonym', 'publishing house', 'city of publication', 'location/street of publication','authority', 'full text source', 'illustrator', 'translator', 'ebook source','more info', 'notes', 'etext publisher', 'ebook no.', 'etext pub date','date accessed', 'editor', 'edition', 'color_word_list','color_word_summary','unnamed: 29', 'unnamed: 30', 'unnamed: 31'], axis=1, inplace=True)\n",
    "#df_combined.insert(8, 'gender', np.nan)\n",
    "#df_combined.insert(9, 'birthdate', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the features gende, publication date, birthyear and nationality provided by the colors corpus seem very promisingn and useful, but are not prvided by any of the other sources, The gap will need to be filled with manual research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined.to_csv('./preprocessing/intermediary_steps/Combined_texts_work_in_progress.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now that it is clear which rows will remain we will do some manual enrichment to fill up the values of the gutenberg data.\n",
    "#df_combined.iloc[:, :10].to_csv('./preprocessing/intermediary_steps/combined_texts_to_be_enriched.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import and finalize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined=pd.read_csv('./preprocessing/intermediary_steps/Combined_texts_work_in_progress.csv')\n",
    "#df_enriched = pd.read_csv('./preprocessing/intermediary_steps/Combined_texts_non_tidy.csv')\n",
    "#df_combined[df_enriched.columns] = df_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the texts poreviously used by Ted Underwood in one of his papers and reduce it to two categories, the stanford gothic collection and the byron and punther gothic selection: Ted Underwood, “The Life Cycles of Genres,” Cultural Analytics May 23, 2016.\n",
    "DOI: 10.22148/16.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p.34f.) The metadata I use for the “Stanford Gothic” were developed at the Stanford Literary Lab; many hands may have been involved, including certainly those of Ryan Heuser and Matthew L. Jockers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stan_df = pd.DataFrame()\n",
    "\n",
    "# for index, row in df_genre.iterrows():\n",
    "#     # Check if 'stangothic' is in 'genretags' column for the row\n",
    "#     tags = [tag.strip() for tag in row['genretags'].split('|')]\n",
    "#     if 'stangothic' in tags:\n",
    "#         # Append the row to stan_df\n",
    "#         stan_df = pd.concat([stan_df, pd.DataFrame(row).T])\n",
    "\n",
    "# # Reset index for the new DataFrame\n",
    "# stan_df = stan_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb_df = pd.DataFrame()\n",
    "\n",
    "# for index, row in df_genre.iterrows():\n",
    "#     # Check if 'pbgothic' is in 'genretags' column for the row\n",
    "#     tags = [tag.strip() for tag in row['genretags'].split('|')]\n",
    "#     if 'pbgothic' in tags:\n",
    "#         # Append the row to pb_df\n",
    "#         pb_df = pd.concat([pb_df, pd.DataFrame(row).T])\n",
    "\n",
    "# # Reset index for the new DataFrame\n",
    "# pb_df = pb_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stan_df.drop(['enumcron', 'date', 'imprint', 'locnum','oclc', 'recordid'], axis=1, inplace=True)\n",
    "#pb_df.drop(['enumcron', 'date', 'imprint', 'locnum','oclc', 'recordid'], axis=1, inplace=True)\n",
    "#pb_df['source'] = 'life cycle: pb'\n",
    "#stan_df['source'] = 'life cycle: stan'\n",
    "#stan_df.rename(columns={'firstpub': 'date'}, inplace=True)\n",
    "#pb_df.rename(columns={'firstpub': 'date'}, inplace=True)\n",
    "#stan_df['nationality'].replace({'uk': 'English', 'ir': 'Irish', 'us': 'American'}, inplace=True)\n",
    "#pb_df['nationality'].replace({'uk': 'English', 'ir': 'Irish', 'us': 'American'}, inplace=True)\n",
    "#pb_df = pb_df[pb_df['date'] <= 1910]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors['Text'] = \"\"\n",
    "# dir_path = \"/Storage/Studium/DigitalHumanities/Semester5/Thesis/code_notebooks/preprocessing/color_corpus/\"\n",
    "\n",
    "\n",
    "# for index, row in df_colors.iterrows():\n",
    "#     if pd.notnull(row['Filename']):  \n",
    "#         file_path = os.path.join(dir_path, row['Filename'])\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             text = file.read()  \n",
    "#         df_colors.at[index, 'Text'] = text \n",
    "\n",
    "\n",
    "# filled_rows = df_colors[df_colors['Text'] != ''].shape[0]\n",
    "# print(filled_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb_df.to_csv('./preprocessing/intermediary_steps/Underwood_punter_selection.csv', index=False)\n",
    "# stan_df.to_csv('./preprocessing/intermediary_steps/Underwood_stanford_selection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_text=pd.read_csv('./preprocessing/intermediary_steps/df_books_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting Mojibake in Project Gutenberg package texts. First we replace the \\n sequences with actual newlines. The decode_match function is used to convert each matched sequence to its actual UTF-8 character. Then a regular expression is used to find all byte-like sequences (e.g., \\xe2) and allow for recognition and removal of these characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(raw_text):\n",
    "    # Convert \\n sequences to actual newlines\n",
    "    text = raw_text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    # Convert byte-like sequences to their actual characters\n",
    "    def decode_match(match):\n",
    "        return bytes.fromhex(match.group(1)).decode('utf-8', errors='replace')\n",
    "    \n",
    "    text = re.sub(r'\\\\x([a-fA-F0-9]{2})', decode_match, text)\n",
    "    if text.startswith(\"b'\"):\n",
    "        text = text[2:]\n",
    "    \n",
    "    # Remove any leading newline characters and still unrecognized bytestring\n",
    "    text = text.lstrip('\\n')\n",
    "    text = re.sub(r'�+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Apply the cleaning function to the  gutenberg books\n",
    "# df_text.loc[df_text['book_id'].notna(), 'text'] = df_text.loc[df_text['book_id'].notna(), 'text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_text['source'].replace({'liste': 'pb-manual', 'colors corpus': 'colors', 'life cycle: pb': 'pb-under', 'bookshelf': 'gutenberg'}, inplace=True)\n",
    "# df_text.drop(['filename', 'subjects','book_id','Unnamed: 0', 'docid', 'genretags'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we intruduce a telling identifier to use for further text identification.\n",
    "The first up to ten letters of both authors last name and title of the text. In the scant cases of overlapping ids we shall ad a distinguishing number at the end  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "# Function to generate the unique value for the index\n",
    "def generate_unique_value(row):\n",
    "    # Extract from author\n",
    "    author_name = remove_punctuation(row['author'].split(',')[0]).replace(' ', '')\n",
    "    author_part = author_name[:10]\n",
    "    \n",
    "    # Extract from title\n",
    "    title_part = remove_punctuation(row['title']).replace(' ', '')\n",
    "    for article in ['A', 'An', 'The']:\n",
    "            title_part = re.sub(r'\\b' + article + r'\\b', '', title_part, flags=re.IGNORECASE)\n",
    "    title_part = title_part[:10]\n",
    "    \n",
    "    return author_part + '_' + title_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the function to each row and store the result in a temporary variable\n",
    "# reference_values = df_text.apply(generate_unique_value, axis=1)\n",
    "\n",
    "# # Insert the reference column as the first column in the DataFrame\n",
    "# df_text.insert(1, 'reference', reference_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_for_duplicates(df):\n",
    "#     # Check for duplicates in the reference column\n",
    "#     duplicates = df[df['reference'].duplicated(keep=False)]\n",
    "\n",
    "#     # Print the duplicates\n",
    "#     if not duplicates.empty:\n",
    "#         print(\"Duplicate values in the reference column:\")\n",
    "#         print(duplicates[['reference']])\n",
    "#     else:\n",
    "#         print(\"No duplicate values found in the reference column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_for_duplicates(df_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets adjust the relevant entries, there were still some duplicate books in there and two books sadly are identical in their reference till the 10th character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values_to_remove = [228, 173, 174, 14]\n",
    "# df_text = df_text[~df_text['index'].isin(values_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the second occurrence of duplicates\n",
    "# second_occurrences = df_text[df_text.duplicated(subset='reference', keep='first')]\n",
    "\n",
    "# # Add '2' to the end of the reference value for these rows\n",
    "# df_text.loc[second_occurrences.index, 'reference'] = second_occurrences['reference'] + '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_for_duplicates(df_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_text.to_csv('./preprocessing/intermediary_steps/df_books_completed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reg=pd.read_csv('./preprocessing/intermediary_steps/df_books_completed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go on to finalize the preprocessing for the following modelling in the subsequent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = df_reg['text'].apply(len).max()\n",
    "# print(f\"The maximum text length in the 'text' column is: {max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The literature suggests that the most important and salient types of words for topic modeling are nouns, verbs, adjectives and adverbs. Those are extracted with the use of a spacy language model. Followed by regex based cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp.max_length = 1640523\n",
    "# # Define the function to preprocess text\n",
    "# def preprocess_text(doc):\n",
    "#     # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "#     doc = nlp(doc)\n",
    "#     # Lower case the text, remove stop words, punctuation and words not chosen for the modeling\n",
    "#     result = []\n",
    "#     for token in doc:\n",
    "#         if token.pos_ in ('NOUN', 'VERB', 'ADJ', 'ADV') and not token.is_stop and not token.is_punct:\n",
    "#             result.append(token.text.lower())\n",
    "#     return ' '.join(result)\n",
    "\n",
    "# # Now apply this function to the 'text' column in the dataframe\n",
    "# df_reg['preprocessed_text'] = df_reg['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_unwanted_elements(text):\n",
    "#     # Define the pattern to match unwanted symbols\n",
    "#     symbols_pattern = re.compile(r\"[+\\-|\\\\\\\"\\“\\[\\]â~▪–◊\\‘‘’\\(\\)\\•€\\•\\\u0014\\,\\’\\;\\—”\\*\\{\\}!?\\./':\\_\\<\\>;=,\u0014\\d+]\")\n",
    "    \n",
    "#     # Define the pattern to match unwanted words\n",
    "#     words_pattern = re.compile(r\"\\b(illustration|use|cost|restriction|restrictions|proofreading|proofread|\"\n",
    "#                                r\"chapter|ebook|ebooks|chapters|contents|author|published|illustrated|publisher|introduction|online|html|\"\n",
    "#                                r\"httpswwwpgdpnet|ὑπ᾽|version|file|original|volume|copyright|copy|volumes)\\b\", re.IGNORECASE)\n",
    "\n",
    "#     # Read the additional unwanted words from a file\n",
    "#     with open('./preprocessing/unwanted_terms.txt', 'r') as file:\n",
    "#         additional_words = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "\n",
    "#     # Create a regex pattern for the additional unwanted words\n",
    "#     additional_words_pattern = re.compile(r'\\b(' + '|'.join(additional_words) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "#     # Remove unwanted symbols\n",
    "#     text = symbols_pattern.sub(\" \", text)\n",
    "    \n",
    "#     # Remove unwanted words\n",
    "#     text = words_pattern.sub(\"\", text)\n",
    "\n",
    "#     # Regex for additional unwanted words - mostly in old greek and latin found in later steps of the processing.\n",
    "#     #Those are too long to be displayed here, so theyx aqre read-in from a file\n",
    "\n",
    "#     text = additional_words_pattern.sub(\"\", text)\n",
    "    \n",
    "#     # Remove extra spaces\n",
    "#     text = re.sub(' +', ' ', text).strip()\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# # Example of applying the function to a dataframe column\n",
    "# df_reg['preprocessed_text'] = df_reg['preprocessed_text'].apply(remove_unwanted_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reg.to_csv('./preprocessing/results/df_books_prep.csv', index=False)\n",
    "df_prep=pd.read_csv('./preprocessing/results/df_books_prep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a final clean up to remove further texts from the selection.\n",
    "After the modeling had been completed, due to unsatisfactory results and a skew in the data, many of the entries from the pb-under, as well as some of the color and many of the color works had been manually re-evaluated and taken out of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove=[\n",
    " 'Armadale',\n",
    " 'The Sketch-Book of Geoffrey Crayon',\n",
    " 'The Alhambra',\n",
    " 'Godolphin',\n",
    " \"King Solomon's Mines\",\n",
    " 'Phantastes',\n",
    " 'Hours Of Solitude', 'Ormond; Or, The Secret Witness', \n",
    " 'A Christmas Carol in Prose; Being a Ghost Story of Christmas',\n",
    " 'Bleak House',\n",
    " 'Great Expectations',\n",
    " 'Oliver Twist',\n",
    " 'She: A History Of Adventure',\n",
    " 'The People of the Mist',\n",
    " 'At the Back of the North Wind',\n",
    " 'St. George and St. Michael, Volume 1',\n",
    " 'The Light Princess',\n",
    " 'The Princess and Curdie',\n",
    " 'A Legend of Montrose',\n",
    " 'Guy Mannering; or, The Astrologer', \n",
    " 'Ivanhoe: A Romance',\n",
    " 'Kenilworth',\n",
    " \"Letters on Demonology and Witchcraft\",\n",
    " 'Old Mortality, Complete',\n",
    " 'Peveril of the Peak',\n",
    " 'Quentin Durward',\n",
    " 'Redgauntlet: A Tale of the Eighteenth Century',\n",
    " 'Rob Roy — Complete',\n",
    " \"St. Ronan's Well\",\n",
    " 'The Antiquary — Complete',\n",
    " 'The Abbot',\n",
    " 'The Betrothed',\n",
    " 'The Bride of Lammermoor',\n",
    " \"The Fair Maid of Perth; Or, St. Valentine's Day\",\n",
    " 'The Fortunes of Nigel',\n",
    " 'The Heart of Mid-Lothian, Complete',\n",
    " 'The Monastery',\n",
    " \"The Surgeon's Daughter\",\n",
    " 'The Talisman',\n",
    " \"Waverley; Or, 'Tis Sixty Years Since\"]\n",
    "\n",
    "#df_prep = df_prep[~df_prep['title'].isin(to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed text into lists of words\n",
    "df_prep['tokenized_text'] = df_prep['preprocessed_text'].apply(lambda text: text.split())\n",
    "#df_prep.to_csv('./preprocessing/results/df_books_prep.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec embeddings, a representation of the language space of the corpus is generated in order to use this as an input for our evaluation metrics in the modeling, as well as input for the ETM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#According to research, the quality for vector representations improves as you increase the vector size until you reach 300 dimensions. After 300 dimensions, the quality of vectors starts to decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Train Word2Vec Model\n",
    "# # # Train the model on the tokenized sentences\n",
    "# word2vec_model = Word2Vec(sentences=df_prep['tokenized_text'].tolist(), vector_size=300, window=10, min_count=1, workers=8)\n",
    "\n",
    "# # # # If you want to save the model to disk\n",
    "# word2vec_model.save(\"./word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.87400156e-01  3.63597661e-01  9.53967869e-02  1.36146903e-01\n",
      "  6.52683750e-02 -3.91167045e-01  2.38470286e-01  7.39968777e-01\n",
      "  5.86055398e-01 -9.86738428e-02 -3.09907436e-01 -9.16796252e-02\n",
      "  4.71953511e-01  2.92291611e-01 -4.55206633e-01 -2.37952515e-01\n",
      "  6.80520851e-03 -1.94612384e-01  7.38793671e-01  2.93383271e-01\n",
      " -2.06774905e-01 -2.27684334e-01 -3.39814901e-01  5.76493703e-02\n",
      "  7.49362350e-01 -2.32862487e-01 -5.62201552e-02 -2.53030598e-01\n",
      "  1.99317724e-01 -2.14907274e-01 -6.57019794e-01  2.35250592e-02\n",
      " -7.10008144e-01 -3.74690443e-01  5.33717200e-02  1.44490167e-01\n",
      "  3.80775511e-01 -1.16112180e-01 -1.98471501e-01 -8.15009996e-02\n",
      " -4.85139340e-01  2.81295776e-02  2.77968466e-01 -5.70377827e-01\n",
      " -3.29397559e-01  3.54064226e-01  6.64371848e-02  5.64452648e-01\n",
      " -2.53516406e-01  5.39948523e-01 -6.51434958e-02 -4.51897323e-01\n",
      " -3.05233300e-01 -4.59913433e-01 -6.84654713e-02  2.69153388e-03\n",
      "  3.27103555e-01  1.51303485e-01 -1.21298328e-01 -3.36090714e-01\n",
      " -4.83732879e-01  2.24577770e-01 -3.26014370e-01 -4.50170115e-02\n",
      "  3.96486282e-01 -3.65934968e-01  2.29305699e-01  9.22877565e-02\n",
      " -5.48429824e-02 -7.19027400e-01  1.22388795e-01  4.99580018e-02\n",
      "  2.03850076e-01 -1.74665649e-03 -6.15881145e-01  2.02150017e-01\n",
      " -4.48896945e-01  1.99548885e-01 -3.34170580e-01  5.95486403e-01\n",
      "  9.28457975e-02 -6.76300466e-01  8.34489167e-01  3.22255827e-02\n",
      " -1.32272050e-01  1.27083987e-01 -4.34984833e-01  5.77331670e-02\n",
      "  2.08956033e-01  1.61211550e-01  5.22046268e-01 -1.30651116e-01\n",
      "  4.60883021e-01  8.16494077e-02  4.93989646e-01  1.65723816e-01\n",
      "  3.44992787e-01 -1.00315899e-01 -6.17311120e-01  5.98827720e-01\n",
      " -1.08771428e-01 -9.65223670e-01  8.50656927e-01 -2.95058191e-01\n",
      "  5.46456039e-01 -1.33648977e-01 -4.88326652e-03  4.02789474e-01\n",
      " -1.07322484e-02  2.97062576e-01 -5.55291951e-01 -4.28529114e-01\n",
      "  2.59927183e-01  8.07762086e-01  5.55154026e-01  3.74313653e-01\n",
      " -2.96862841e-01  5.10337174e-01  5.60580432e-01 -1.19672254e-01\n",
      "  6.00529253e-01  1.77555174e-01  2.68931031e-01 -1.67640746e-02\n",
      "  2.42678095e-02  7.16721043e-02  5.81104279e-01 -1.84345663e-01\n",
      " -3.54693770e-01 -2.58687083e-02 -5.08208871e-01  4.84709114e-01\n",
      " -2.03549504e-01 -2.72275746e-01  3.27258587e-01  6.01545691e-01\n",
      "  1.55964181e-01 -5.24784803e-01 -1.74654052e-01 -6.43637419e-01\n",
      " -1.51754953e-02 -3.41613293e-01  1.98907137e-01  2.45910004e-01\n",
      "  5.60909927e-01 -1.24447778e-01 -6.73812330e-01 -3.15159746e-02\n",
      "  4.95574564e-01 -9.14602131e-02  6.15336299e-01 -8.38087022e-01\n",
      " -3.83393288e-01 -1.05536424e-01  1.70029774e-01 -3.21162909e-01\n",
      " -2.65948445e-01  1.47736803e-01  6.62794054e-01  3.03202152e-01\n",
      " -6.15772754e-02  1.67539343e-01 -6.32475376e-01  5.52159548e-01\n",
      " -8.37674141e-02  3.59309882e-01 -6.18261211e-02 -2.22689718e-01\n",
      "  5.14163196e-01  5.83682418e-01  2.89020449e-01  2.58361429e-01\n",
      "  4.62115437e-01 -2.47972354e-01 -8.43182981e-01  2.70439595e-01\n",
      " -1.21998368e-02 -6.97058678e-01 -6.10297620e-01 -3.57511878e-01\n",
      " -3.01906705e-01  3.28770697e-01 -2.20689133e-01 -3.01161349e-01\n",
      "  1.37058854e-01  7.69706309e-01  5.34628749e-01 -2.11039320e-01\n",
      "  2.81650156e-01 -5.78221500e-01 -5.67640424e-01 -9.39728767e-02\n",
      " -5.35979152e-01  2.30143711e-01  3.67872953e-01 -9.00316715e-01\n",
      "  4.83984828e-01 -4.91745859e-01 -6.17337488e-02  1.98387697e-01\n",
      " -3.44219744e-01 -1.27484649e-03  2.01181561e-01 -1.04943141e-01\n",
      " -7.16504097e-01 -3.37424576e-02 -5.17251194e-01 -9.53595042e-02\n",
      "  2.59930789e-01 -3.37784737e-01  3.35788995e-01 -4.48472798e-01\n",
      " -1.06492531e+00  3.28426287e-02  2.21173882e-01 -2.14307249e-01\n",
      " -6.49056196e-01 -2.72456050e-01 -4.05343175e-01 -5.58132589e-01\n",
      "  4.93515611e-01  9.81374308e-02 -3.02693635e-01 -1.81432486e-01\n",
      " -4.79328901e-01 -5.39891422e-01  1.63197517e-02 -2.96862662e-01\n",
      " -7.27285445e-01 -3.15663256e-02  3.01897526e-01 -8.14774215e-01\n",
      " -2.02014089e-01  2.65434980e-01  1.17220327e-01 -3.78246903e-01\n",
      " -2.75616109e-01  4.83017594e-01  6.85966462e-02 -3.08753759e-01\n",
      "  1.83665246e-01 -4.46762861e-04 -7.04556108e-01 -3.04780714e-02\n",
      " -1.38077393e-01 -5.35294831e-01 -9.94290411e-02  5.92000663e-01\n",
      "  8.12375769e-02  8.26339349e-02  2.72214979e-01  9.14443806e-02\n",
      "  2.29964554e-01  2.78085917e-01 -3.00542861e-01  3.35615575e-01\n",
      "  4.98034954e-01  6.33791924e-01 -6.53109372e-01 -2.64770538e-01\n",
      " -2.91193742e-02  3.26866060e-01 -1.82360262e-01 -8.85515332e-01\n",
      " -2.40380406e-01  2.10291401e-01 -3.32346139e-03  7.46673718e-02\n",
      "  4.75635648e-01  5.51923886e-02 -1.60784602e-01  8.54162499e-02\n",
      " -1.74284235e-01  2.31493175e-01  4.76191312e-01  5.81296673e-03\n",
      "  5.79107031e-02 -3.69462162e-01  1.45074159e-01 -2.45889034e-02\n",
      "  2.39252560e-02 -5.75524643e-02 -2.74673343e-01  7.13665336e-02\n",
      " -3.11227500e-01 -2.60104556e-02 -5.77045977e-01  2.02265367e-01\n",
      "  1.16648383e-01  5.75569332e-01  6.27159849e-02  1.29331917e-01\n",
      " -1.43592715e-01 -3.08875497e-02 -4.58984897e-02  6.71775401e-01\n",
      " -5.16596716e-03 -4.69771594e-01  4.32941988e-02 -1.11748360e-01]\n"
     ]
    }
   ],
   "source": [
    "# word2vec_model = Word2Vec.load(\"./word2vec/word2vec_model\")\n",
    "# word_vectors = word2vec_model.wv\n",
    "# print(word_vectors['human']) #looking good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#word2vec_model=Word2Vec.load(\"./word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length_dataframe(df):\n",
    "    # Create a new DataFrame with the required columns\n",
    "    length_df = pd.DataFrame({\n",
    "        'reference': df['reference'],\n",
    "        'len(text)': df['text'].apply(lambda x: len(str(x).split())),\n",
    "        'len(preprocessed_text)': df['preprocessed_text'].apply(lambda x: len(str(x).split()))\n",
    "    })\n",
    "    \n",
    "    return length_df\n",
    "\n",
    "df_length=create_length_dataframe(df_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are finally seperated into 5000 word chunks in order to improve the quality of the the topic modeling, a common practice recommended in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size):\n",
    "    words = text.split()  # Split text into words\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield ' '.join(words[i:i + chunk_size])\n",
    "\n",
    "chunks_data = []\n",
    "\n",
    "# Iterate over the DataFrame and chunk the text\n",
    "for index, row in df_prep.iterrows():\n",
    "    text_chunks = list(chunk_text(row['preprocessed_text'], 5000))\n",
    "    for i, chunk in enumerate(text_chunks, 1):\n",
    "        chunks_data.append({\n",
    "            'preprocessed_text': chunk,\n",
    "            'reference': f\"{row['reference']}_{i}\"\n",
    "        })\n",
    "\n",
    "# Create the new DataFrame from the list of data\n",
    "df_chunk = pd.DataFrame(chunks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk.to_csv('./preprocessing/results/df_books_chunk.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
