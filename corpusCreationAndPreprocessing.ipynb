{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gutenbergpy.gutenbergcache import GutenbergCache, GutenbergCacheSettings, GutenbergCacheTypes\n",
    "#import gutenbergpy.textget \n",
    "from gensim.models import Word2Vec\n",
    "from joblib import dump\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and cache the Project Gutenberg Metadata to query it\n",
    "#GutenbergCache.create() #refresh=False, download=True, unpack=True, parse=True, cache=True, deleteTemp=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache = GutenbergCache.get_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor=cache.native_query(\"\"\"\n",
    "#                           SELECT \n",
    "#                            MAX(b.gutenbergbookid) as book_id,\n",
    "#                            CASE \n",
    "#                                 WHEN MAX(a.name) = 'Boz' THEN 'Dickens, Charles'\n",
    "#                                 WHEN MAX(a.name) = 'Marshall, William' THEN 'Walpole, Horace'\n",
    "#                                 WHEN MAX(a.name) = 'Grile, Dod' THEN 'Bierce, Ambrose'\n",
    "#                                 ELSE MAX(a.name) \n",
    "#                             END as authors,\n",
    "#                             t.name as titles\n",
    "#                           FROM books b \n",
    "#                           join languages l on l.id=b.languageid and l.name='en'\n",
    "#                           join book_subjects bsu on b.id=bsu.bookid\n",
    "#                           join bookshelves bsh on b.bookshelveid=bsh.id\n",
    "#                           join subjects s on s.id=bsu.subjectid\n",
    "#                           join titles t on t.bookid=b.id\n",
    "#                           join book_authors ba on ba.bookid=b.id\n",
    "#                           join authors a on ba.authorid=a.id\n",
    "#                           where \n",
    "#                                 (lower(bsh.name) like '%horror%'\n",
    "#                                 or lower(s.name) like '%horror%'\n",
    "#                                 or lower(bsh.name) like '%gothic%'\n",
    "#                                 or lower(s.name) like '%gothic%'\n",
    "#                                 or lower(bsh.name) like '%supernatural%'\n",
    "#                                 or lower(s.name) like '%supernatural%'\n",
    "#                                 or lower(bsh.name) like '%paranormal%'\n",
    "#                                 or lower(s.name) like '%paranormal%'\n",
    "#                                 or lower(bsh.name) like '%vampire%'\n",
    "#                                 or lower(s.name) like '%vampire%'\n",
    "#                                 or lower(bsh.name) like '%ghost%'\n",
    "#                                 or lower(s.name) like '%ghost%')\n",
    "#                                 and a.name not in \n",
    "#                                 ('Arthur, Robert','Baker, Frank','Baldwin, Edward','Birkhead, Edith',\n",
    "#                                 'Blackwood, Algernon','Bloxam, Matthew Holbeche','De Quincey, Thomas',\n",
    "#                                 'DeQuincey, Thomas',\n",
    "#                                 'De Vet, Charles V.','Glad, Victoria','Hammond, Keith', 'Hodgson, William Hope',\n",
    "#                                 'Hopkins, R. Thurston (Robert Thurston)','Kafka, F. (Franz)','La Spina, Greye',\n",
    "#                                 'Leroux, Gaston','Littlewit, Humphrey','Marks, Winston K. (Winston Kinney)',\n",
    "#                                 'O''Donnell, Elliot', 'Oliver, George', 'Olivieri, David', 'Kafka, Franz',\n",
    "#                                 'Peterson, Don', 'Lovecraft, H. P. (Howard Phillips)','Phillips, Forbes',\n",
    "#                                 'Tenneshaw, S. M.','Weinbaum, Stanley G. (Stanley Grauman)')\n",
    "#                                 and b.id not in (12728, 12739, 12751, 12762) -- collected works\n",
    "#                                 and b.gutenbergbookid not in (2147, 31469, 50133, 50133,2040, 42324, 41445, 9629,\n",
    "#                                 50133, 24022, 20673, 20038, 13334, 6542, 19505, 19337, 18233,20034,14317,\n",
    "#                                 14168,32076,18233,20034,14317, 24350, 25037) -- ill-fitting titles\n",
    "#                             GROUP BY t.name\n",
    "#                           \"\"\")\n",
    "\n",
    "# results = cursor.fetchall()\n",
    "# df = pd.DataFrame(results, columns=[column[0] for column in cursor.description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the package does not fetch biographical data on the authors or original publication dates, the filtering process had to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function downloads a book by its Gutenberg ID\n",
    "# def download_book(book_id):\n",
    "#     raw_book = gutenbergpy.textget.get_text_by_id(book_id)  # with headers\n",
    "#     clean_book = gutenbergpy.textget.strip_headers(raw_book)  # without headers\n",
    "#     return clean_book\n",
    "\n",
    "# df['text'] = ''\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     try:\n",
    "#         df.loc[idx, 'text'] = download_book(row['book_id'])\n",
    "#     except Exception:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./preprocessing/corpora/Gutenberg_texts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of names obtained from the Table of contents of David Punters and Glennis Byrons book the Gothic filtered for all authors that were active before the beginning of the 20th centuary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names= ['William Harrison Ainsworth','William Beckford','E. F. Benson','Ambrose Bierce','Elizabeth Bowen','Mary Elizabeth Braddon',\n",
    "# 'Charles Brockden Brown','Robert W. Chambers','Wilkie Collins','Marie Corelli','Charlotte Dacre','Walter de la Mare',\n",
    "# 'Isak Dinesen','Elizabeth Gaskell','William Godwin','H. Rider Haggard','Nathaniel Hawthorne', 'William Hope Hodgson',\n",
    "# 'James Hogg','Washington Irving','G. P. R. James','Henry James','Francis Lathom','J. Sheridan Le Fanu','Sophia Lee','Vernon Lee','Matthew Lewis',\n",
    "# 'Bulwer Lytton','George MacDonald','Arthur Machen', 'James Macpherson','Charles Robert Maturin','Herman Melville','Margaret Oliphant','Edgar Allan Poe',\n",
    "# 'John Polidori','Ann Radcliffe','Clara Reeve','G. W. M. Reynolds','Walter Scott','Mary Wollstonecraft Shelley','Charlotte Smith','Tobias Smollett',\n",
    "# 'Robert Louis Stevenson','Bram Stoker','Horace Walpole']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted_names = []\n",
    "# for name in names:\n",
    "#     name_parts = name.split(\" \")\n",
    "#     last_name = name_parts[-1]\n",
    "#     other_names = name_parts[:-1]\n",
    "#     formatted_name = last_name + ', ' + \" \".join(other_names)\n",
    "#     formatted_names.append(formatted_name)\n",
    "\n",
    "# names_string = ', '.join(f\"'{name}'\" for name in formatted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor=cache.native_query(f\"\"\"\n",
    "#                           SELECT \n",
    "#                             MAX(b.gutenbergbookid) as book_id,\n",
    "#                             MAX(a.name) as authors,\n",
    "#                             t.name as titles\n",
    "#                           FROM books b \n",
    "#                           join languages l on l.id=b.languageid and l.name='en'\n",
    "#                           join book_subjects bsu on b.id=bsu.bookid\n",
    "#                           join bookshelves bsh on b.bookshelveid=bsh.id\n",
    "#                           join subjects s on s.id=bsu.subjectid\n",
    "#                           join titles t on t.bookid=b.id\n",
    "#                           join book_authors ba on ba.bookid=b.id\n",
    "#                           join authors a on ba.authorid=a.id\n",
    "#                           where \n",
    "#                           (a.name in ({names_string}) or t.name in ('Wuthering Heights', 'Hieroglyphic Tales', 'Dreams, Waking Thoughts, and Incidents','Vathek; An Arabian Tale',\n",
    "#                           'Melmoth the Wanderer, Vol. 1','Melmoth the Wanderer, Vol. 2','Melmoth the Wanderer, Vol. 3','A Sicilian Romance','The Evil Guest',\n",
    "#                           'Melmoth the Wanderer, Vol. 4','Auriol; or, The Elixir of Life', 'The Children of the Abbey: A Tale', 'The Dead Secret: A Novel',\n",
    "#                           'The Wyvern mystery', 'A Stable for Nightmares; or, Weird Tales', 'The House by the Church-Yard', 'Green Tea; Mr. Justice Harbottle',\n",
    "#                           'After Dark', 'The Haunted Hotel: A Mystery of Modern Venice', 'Armadale', 'The Queen of Hearts', \n",
    "#                           'The Frozen Deep', 'The Dead Secret: A Novel', 'The Legacy of Cain', 'The Black Robe','Heart and Science: A Story of the Present Time',\n",
    "#                           'Phantom Fortune, a Novel', 'The Little Red Foot', 'In Search of the Unknown', 'The Moonlit Way: A Novel', 'The Slayer of Souls',\n",
    "#                           'The Hidden Children', 'Phantastes: A Faerie Romance for Men and Women', 'The Old English Baron: a Gothic Story', 'Wagner, the Wehr-Wolf',\n",
    "#                           'Emmeline, the Orphan of the Castle', 'The House on the Borderland', 'Carnacki, the Ghost Finder', 'A Phantom Lover', 'The People of the Mist',\n",
    "#                           'The Sorrows of Satan', 'The soul of Lilith'))\n",
    "#                           and a.name not in ('Mare, Walter de la', 'Melville, Herman','Arthur, Robert','Baker, Frank',\n",
    "#                           'Baldwin, Edward','Birkhead, Edith','Blackwood, Algernon','Bloxam, Matthew Holbeche',\n",
    "#                           'De Quincey, Thomas','DeQuincey, Thomas','De Vet, Charles V.','Glad, Victoria',\n",
    "#                           'Hammond, Keith', 'Hodgson, William Hope','Mare, Walter de la',\n",
    "#                           'Hopkins, R. Thurston (Robert Thurston)','Kafka, F. (Franz)','La Spina, Greye',\n",
    "#                           'Leroux, Gaston','Littlewit, Humphrey','Marks, Winston K. (Winston Kinney)',\n",
    "#                           'O''Donnell, Elliot', 'Oliver, George', 'Olivieri, David', 'Kafka, Franz',\n",
    "#                           'Peterson, Don', 'Lovecraft, H. P. (Howard Phillips)','Phillips, Forbes',\n",
    "#                           'Tenneshaw, S. M.','Weinbaum, Stanley G. (Stanley Grauman)')\n",
    "#                           and b.id not in (12728, 12739, 12751, 12762) \n",
    "#                           and b.gutenbergbookid not in (2147,8939, 8940,7023,7024, 31469, 50133, 50133,2040, 42324, 41445, 9629,\n",
    "#                           50133, 24022, 20673, 20038, 13334, 6542,4964, 4965,4966, 864,25611,14535,19505, 19337, 18233,20034,14317,12396,12397,120,\n",
    "#                           14168,32076,18233,20034,14317, 24350, 25037, 13334, 14082, 6942, 6943,2590, 5355, 5354, 5353, 6661,616,26458,\n",
    "#                           13334,9377, 771, 30486, 3606, 15697, 20656, 2834, 2833, 12031, 12032,23545, 12031, 589, 382, 421,441,322, 25617,848)\n",
    "#                           and t.name not in ('The Star-Chamber: An Historical Romance, Volume 2', 'The Star-Chamber: An Historical Romance, Volume 1',\n",
    "#                           'True Stories of History and Biography','The Scarlet Letter','Twice-told tales', 'Old Christmas: from the Sketch Book of Washington Irving',\n",
    "#                           'Astoria; Or, Anecdotes of an Enterprise Beyond the Rocky Mountains','The Portrait of a Lady — Volume 1',\n",
    "#                           'The Portrait of a Lady — Volume 1', 'The Light Princess and Other Fairy Stories')\n",
    "#                           GROUP BY t.name\n",
    "#                           ORDER BY a.name, t.name\n",
    "#                           \"\"\")\n",
    "\n",
    "# results = cursor.fetchall()\n",
    "# df = pd.DataFrame(results, columns=[column[0] for column in cursor.description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function downloads a book by its Gutenberg ID\n",
    "# def download_book(book_id):\n",
    "#     raw_book = gutenbergpy.textget.get_text_by_id(book_id)  # with headers\n",
    "#     clean_book = gutenbergpy.textget.strip_headers(raw_book)  # without headers\n",
    "#     return clean_book\n",
    "\n",
    "# df['text'] = ''\n",
    "\n",
    "# for idx, row in df.iterrows():\n",
    "#     try:\n",
    "#         df.loc[idx, 'text'] = download_book(row['book_id'])\n",
    "#     except Exception:\n",
    "#         continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_init=pd.read_csv('./preprocessing/corpora/Gutenberg_texts.csv')\n",
    "# df['source'] = 'liste'\n",
    "# df_init['source'] = 'bookshelf'\n",
    "# union_df = pd.concat([df, df_init])\n",
    "# sorted_df = union_df.sort_values('authors')\n",
    "# unique_df = sorted_df.drop_duplicates(subset='titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_df.to_csv('./preprocessing/corpora/Gutenberg_texts_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gutenberg=pd.read_csv('./preprocessing/corpora/Gutenberg_texts_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the meager amount of project Gutenberg, additional sources shall be considered. Given the quality of the texts and the excessive paratexts, some manual cleaning will be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gothic colors corpus contains a number of intersting texts, as well as highly relevant metadata, we shall make us of here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shall be restricted to relevant texts, joined with our existing dataframe and the files read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors=pd.read_csv('./preprocessing/corpora/gothic_texts.csv')\n",
    "# df_colors = df_colors[~(df_colors['Nationality'].isin(['French', 'German', 'Italian', 'Sicilian', 'Swiss']) | \n",
    "#            df_colors['Genre'].isin(['Aesthetic Theory','Criticism', 'Literary Theory', 'Memoir/Biography', 'Review', 'Travel Writing']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors['Text'] = \"\"\n",
    "# dir_path = \"/Storage/Studium/DigitalHumanities/Semester5/Thesis/code_notebooks/color_corpus/corpora/\"\n",
    "\n",
    "\n",
    "# for index, row in df_colors.iterrows():\n",
    "#     if pd.notnull(row['Filename']):  \n",
    "#         file_path = os.path.join(dir_path, row['Filename'])\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             text = file.read()  \n",
    "#         df_colors.at[index, 'Text'] = text \n",
    "\n",
    "\n",
    "# filled_rows = df_colors[df_colors['Text'] != ''].shape[0]\n",
    "# print(filled_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalization to combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors.columns = df_colors.columns.str.lower()\n",
    "# df_colors['source'] = 'colors corpus'\n",
    "# df_gutenberg = df_gutenberg.rename(columns={'authors': 'author','titles': 'title'})\n",
    "# df_combined = pd.concat([df_colors, df_gutenberg])\n",
    "# df_combined['text'].replace('', np.nan, inplace=True)\n",
    "# df_combined['text_filled'] = df_combined['text'].notna().astype(int)\n",
    "# df_combined[\"sort_helper\"] = df_combined[\"source\"].apply(lambda x: 0 if x == \"colors corpus\" else 1)\n",
    "# df_combined = df_combined.sort_values(by=['text_filled', 'author', 'title', 'sort_helper'], ascending=[False, True, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Export of the first five columns for manual correction before unification\n",
    "#df_combined.iloc[:, :5].to_csv('./preprocessing/intermediary_steps/Combined_texts_non_tidy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the data back into a DataFrame\n",
    "# df_imported = pd.read_csv('./preprocessing/intermediary_steps/Combined_texts_non_tidy.csv')\n",
    "# df_combined[df_imported.columns] = df_imported\n",
    "# df_combined = df_combined.sort_values(by=['text_filled', 'author', 'title', 'sort_helper'], ascending=[False, True, True, True])\n",
    "# df_combined = df_combined.drop_duplicates(subset=['author', 'title'], keep='first')\n",
    "# df_combined.drop(['text_filled', 'sort_helper'], axis=1, inplace=True)\n",
    "# df_combined = df_combined.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined.drop(['publisher', 'pseudonym', 'publishing house', 'city of publication', 'location/street of publication','authority', 'full text source', 'illustrator', 'translator', 'ebook source','more info', 'notes', 'etext publisher', 'ebook no.', 'etext pub date','date accessed', 'editor', 'edition', 'color_word_list','color_word_summary','unnamed: 29', 'unnamed: 30', 'unnamed: 31'], axis=1, inplace=True)\n",
    "#df_combined.insert(8, 'gender', np.nan)\n",
    "#df_combined.insert(9, 'birthdate', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined.to_csv('./preprocessing/intermediary_steps/Combined_texts_work_in_progress.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Now that it is clear which rows will remain we will do some manual enrichment to fill up the values of the gutenberg data.\n",
    "#df_combined.iloc[:, :10].to_csv('./preprocessing/intermediary_steps/combined_texts_to_be_enriched.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import and finalize the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_combined=pd.read_csv('./preprocessing/intermediary_steps/Combined_texts_work_in_progress.csv')\n",
    "#df_enriched = pd.read_csv('./preprocessing/intermediary_steps/Combined_texts_non_tidy.csv')\n",
    "#df_combined[df_enriched.columns] = df_enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data used by Ted Underwood in one of his papers and reduce it to two categories, the stanford gothic collection and the byron and punther gothic selection: Ted Underwood, “The Life Cycles of Genres,” Cultural Analytics May 23, 2016.\n",
    "DOI: 10.22148/16.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before 1840 I relied heavily on the Stanford\n",
    "Literary Lab’s list of Gothic fiction; 35 after 1940 I relied increasingly on the Library of Congress genre tags associated with “horror” or the “ghost story.” The metadata I use for the “Stanford Gothic” were developed at the Stanford Literary Lab; many hands may have been involved, including certainly those of Ryan Heuser and Matthew L. Jockers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating an empty DataFrame to store the rows where 'genretags' contains 'stangothic'\n",
    "# stan_df = pd.DataFrame()\n",
    "\n",
    "# # Loop through each row in the DataFrame\n",
    "# for index, row in df_genre.iterrows():\n",
    "#     # Check if 'stangothic' is in 'genretags' column for the row\n",
    "#     tags = [tag.strip() for tag in row['genretags'].split('|')]\n",
    "#     if 'stangothic' in tags:\n",
    "#         # Append the row to stan_df\n",
    "#         stan_df = pd.concat([stan_df, pd.DataFrame(row).T])\n",
    "\n",
    "# # Reset index for the new DataFrame\n",
    "# stan_df = stan_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating an empty DataFrame to store the rows where 'genretags' contains 'stangothic'\n",
    "# pb_df = pd.DataFrame()\n",
    "\n",
    "# # Loop through each row in the DataFrame\n",
    "# for index, row in df_genre.iterrows():\n",
    "#     # Check if 'pbgothic' is in 'genretags' column for the row\n",
    "#     tags = [tag.strip() for tag in row['genretags'].split('|')]\n",
    "#     if 'pbgothic' in tags:\n",
    "#         # Append the row to stan_df\n",
    "#         pb_df = pd.concat([pb_df, pd.DataFrame(row).T])\n",
    "\n",
    "# # Reset index for the new DataFrame\n",
    "# pb_df = pb_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stan_df.drop(['enumcron', 'date', 'imprint', 'locnum','oclc', 'recordid'], axis=1, inplace=True)\n",
    "#pb_df.drop(['enumcron', 'date', 'imprint', 'locnum','oclc', 'recordid'], axis=1, inplace=True)\n",
    "#pb_df['source'] = 'life cycle: pb'\n",
    "#stan_df['source'] = 'life cycle: stan'\n",
    "#stan_df.rename(columns={'firstpub': 'date'}, inplace=True)\n",
    "#pb_df.rename(columns={'firstpub': 'date'}, inplace=True)\n",
    "#stan_df['nationality'].replace({'uk': 'English', 'ir': 'Irish', 'us': 'American'}, inplace=True)\n",
    "#pb_df['nationality'].replace({'uk': 'English', 'ir': 'Irish', 'us': 'American'}, inplace=True)\n",
    "#pb_df = pb_df[pb_df['date'] <= 1910]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_colors['Text'] = \"\"\n",
    "# dir_path = \"/Storage/Studium/DigitalHumanities/Semester5/Thesis/code_notebooks/preprocessing/color_corpus/\"\n",
    "\n",
    "\n",
    "# for index, row in df_colors.iterrows():\n",
    "#     if pd.notnull(row['Filename']):  \n",
    "#         file_path = os.path.join(dir_path, row['Filename'])\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             text = file.read()  \n",
    "#         df_colors.at[index, 'Text'] = text \n",
    "\n",
    "\n",
    "# filled_rows = df_colors[df_colors['Text'] != ''].shape[0]\n",
    "# print(filled_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb_df.to_csv('./preprocessing/intermediary_steps/Underwood_punter_selection.csv', index=False)\n",
    "# stan_df.to_csv('./preprocessing/intermediary_steps/Underwood_stanford_selection.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_text=pd.read_csv('./preprocessing/intermediary_steps/df_books_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting Mojibake in Project Gutenberg package texts. This code first replaces the \\n sequences with actual newlines. Then, it uses a regular expression to find all byte-like sequences (e.g., \\xe2) and decodes them as UTF-8 characters. The decode_match function is used to convert each matched sequence to its actual character. While this is not entirely successful, it allows for ease of recognition and removal of these characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text\n",
    "def clean_text(raw_text):\n",
    "    # Convert \\n sequences to actual newlines\n",
    "    text = raw_text.replace('\\\\n', '\\n')\n",
    "    \n",
    "    # Convert byte-like sequences to their actual characters\n",
    "    def decode_match(match):\n",
    "        return bytes.fromhex(match.group(1)).decode('utf-8', errors='replace')\n",
    "    \n",
    "    text = re.sub(r'\\\\x([a-fA-F0-9]{2})', decode_match, text)\n",
    "    if text.startswith(\"b'\"):\n",
    "        text = text[2:]\n",
    "    \n",
    "    # Remove any leading newline characters and still unrecognized bytestring\n",
    "    text = text.lstrip('\\n')\n",
    "    text = re.sub(r'�+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Apply the cleaning function to the  gutenberg books\n",
    "# df_text.loc[df_text['book_id'].notna(), 'text'] = df_text.loc[df_text['book_id'].notna(), 'text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_text['source'].replace({'liste': 'pb-manual', 'colors corpus': 'colors', 'life cycle: pb': 'pb-under', 'bookshelf': 'gutenberg'}, inplace=True)\n",
    "# df_text.drop(['filename', 'subjects','book_id','Unnamed: 0', 'docid', 'genretags'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we intruduce a telling identifier to use for further text identification.\n",
    "The first up to ten letters of both authros last name and title of the text. In the scant cases of overlapping ids we shall ad a distinguishing number at the end  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation from a string\n",
    "def remove_punctuation(s):\n",
    "    return re.sub(r'[^\\w\\s]', '', s)\n",
    "\n",
    "# Function to generate the unique value for the index\n",
    "def generate_unique_value(row):\n",
    "    # Extract from author\n",
    "    author_name = remove_punctuation(row['author'].split(',')[0]).replace(' ', '')\n",
    "    author_part = author_name[:10]\n",
    "    \n",
    "    # Extract from title\n",
    "    title_part = remove_punctuation(row['title']).replace(' ', '')\n",
    "    for article in ['A', 'An', 'The']:\n",
    "            title_part = re.sub(r'\\b' + article + r'\\b', '', title_part, flags=re.IGNORECASE)\n",
    "    title_part = title_part[:10]\n",
    "    \n",
    "    return author_part + '_' + title_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apply the function to each row and store the result in a temporary variable\n",
    "# reference_values = df_text.apply(generate_unique_value, axis=1)\n",
    "\n",
    "# # Insert the reference column as the first column in the DataFrame\n",
    "# df_text.insert(1, 'reference', reference_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_for_duplicates(df):\n",
    "#     # Check for duplicates in the reference column\n",
    "#     duplicates = df[df['reference'].duplicated(keep=False)]\n",
    "\n",
    "#     # Print the duplicates\n",
    "#     if not duplicates.empty:\n",
    "#         print(\"Duplicate values in the reference column:\")\n",
    "#         print(duplicates[['reference']])\n",
    "#     else:\n",
    "#         print(\"No duplicate values found in the reference column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_for_duplicates(df_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets adjust the relevant entries, there were still some duplicate books in there and two books sadly are identical in their reference till the 10th character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values_to_remove = [228, 173, 174, 14]\n",
    "# df_text = df_text[~df_text['index'].isin(values_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the second occurrence of duplicates\n",
    "# second_occurrences = df_text[df_text.duplicated(subset='reference', keep='first')]\n",
    "\n",
    "# # Add '2' to the end of the reference value for these rows\n",
    "# df_text.loc[second_occurrences.index, 'reference'] = second_occurrences['reference'] + '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_for_duplicates(df_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_text.to_csv('./preprocessing/intermediary_steps/df_books_completed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reg=pd.read_csv('./preprocessing/intermediary_steps/df_books_completed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go on to final preprocessing for the following modelling in the subsequent notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = df_reg['text'].apply(len).max()\n",
    "# print(f\"The maximum text length in the 'text' column is: {max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "# nlp.max_length = 1640523\n",
    "# # Define the function to preprocess text\n",
    "# def preprocess_text(doc):\n",
    "#     # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "#     doc = nlp(doc)\n",
    "#     # Lower case the text, remove stop words, punctuation and words not chosen for the modeling\n",
    "#     result = []\n",
    "#     for token in doc:\n",
    "#         if token.pos_ in ('NOUN', 'VERB', 'ADJ', 'ADV') and not token.is_stop and not token.is_punct:\n",
    "#             result.append(token.text.lower())\n",
    "#     return ' '.join(result)\n",
    "\n",
    "# # Now apply this function to the 'text' column in the dataframe\n",
    "# df_reg['preprocessed_text'] = df_reg['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_unwanted_elements(text):\n",
    "#     # Define the pattern to match unwanted symbols\n",
    "#     symbols_pattern = re.compile(r\"[+\\-|\\\\\\\"\\“\\[\\]â~▪–◊\\‘‘’\\(\\)\\•€\\•\\\u0014\\,\\’\\;\\—”\\*\\{\\}!?\\./':\\_\\<\\>;=,\u0014\\d+]\")\n",
    "    \n",
    "#     # Define the pattern to match unwanted words\n",
    "#     words_pattern = re.compile(r\"\\b(illustration|use|cost|restriction|restrictions|proofreading|proofread|\"\n",
    "#                                r\"chapter|ebook|ebooks|chapters|contents|author|published|illustrated|publisher|introduction|online|html|\"\n",
    "#                                r\"httpswwwpgdpnet|ὑπ᾽|version|file|original|volume|copyright|copy|volumes)\\b\", re.IGNORECASE)\n",
    "\n",
    "#     # Read the additional unwanted words from a file\n",
    "#     with open('./preprocessing/unwanted_terms.txt', 'r') as file:\n",
    "#         additional_words = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "\n",
    "#     # Create a regex pattern for the additional unwanted words\n",
    "#     additional_words_pattern = re.compile(r'\\b(' + '|'.join(additional_words) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "#     # Remove unwanted symbols\n",
    "#     text = symbols_pattern.sub(\" \", text)\n",
    "    \n",
    "#     # Remove unwanted words\n",
    "#     text = words_pattern.sub(\"\", text)\n",
    "\n",
    "#     # Regex for additional unwanted words - mostly in old greek and latin found in later steps of the processing.\n",
    "#     #Those are too long to be displayed here, so theyx aqre read-in from a file\n",
    "\n",
    "#     text = additional_words_pattern.sub(\"\", text)\n",
    "    \n",
    "#     # Remove extra spaces\n",
    "#     text = re.sub(' +', ' ', text).strip()\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# # Example of applying the function to a dataframe column\n",
    "# df_reg['preprocessed_text'] = df_reg['preprocessed_text'].apply(remove_unwanted_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reg.to_csv('./preprocessing/results/df_books_prep.csv', index=False)\n",
    "df_prep=pd.read_csv('./preprocessing/results/df_books_prep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed text into lists of words\n",
    "df_prep['tokenized_text'] = df_prep['preprocessed_text'].apply(lambda text: text.split())\n",
    "#df_prep.to_csv('./preprocessing/results/df_books_prep.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#According to research, the quality for vector representations improves as you increase the vector size until you reach 300 dimensions. After 300 dimensions, the quality of vectors starts to decrease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Train Word2Vec Model\n",
    "# # # Train the model on the tokenized sentences\n",
    "# word2vec_model = Word2Vec(sentences=df_prep['tokenized_text'].tolist(), vector_size=300, window=10, min_count=1, workers=8)\n",
    "\n",
    "# # # # If you want to save the model to disk\n",
    "# word2vec_model.save(\"./word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.36965343  0.2943892  -0.05698197  0.6574972  -0.24417302 -0.754673\n",
      "  0.37357613  0.6892637   0.3379611   0.5534327  -0.57929724 -0.9303663\n",
      "  0.16028324  0.17605315 -0.63722086 -0.7515705   0.37558553 -0.65915084\n",
      "  0.9814807   0.2902053  -0.8662499  -0.25317514  0.2724193   0.49405015\n",
      "  1.0600656   0.08558165 -0.24783744 -0.3200631  -0.35618174  0.01094976\n",
      " -0.03929979 -0.5572171  -0.89015836 -0.5205475   0.14081764 -0.5854277\n",
      "  0.25566748  0.14684212 -0.41253257 -0.05313004 -0.27145353  0.05069672\n",
      " -0.71033955 -0.06984958  0.1160382   0.6501742  -0.50770813  0.17445906\n",
      "  0.36772546  0.31145844 -0.27868843 -0.3138634  -0.83875084  0.00495603\n",
      " -0.12454858  0.4086173   0.26589274 -0.44615158 -0.08347578 -0.0042361\n",
      "  0.05588238  0.18997246  0.16160488  0.20437227  0.33096567 -1.2726581\n",
      "  0.18016215  0.15498151 -0.26404506 -0.43492582 -0.2996518   0.19696808\n",
      "  0.6037472  -0.74893844 -0.67219657  0.4363479  -0.64522564  0.42433968\n",
      "  0.01955164  0.3462619  -0.01731746 -0.87323     0.6960736   0.81679094\n",
      "  1.0078005  -0.6332595  -1.0262489  -0.15995045  0.19994557  0.16443452\n",
      "  0.24402261 -0.081077   -0.06695416 -0.18965283  1.0304904   0.44378594\n",
      "  0.5401137  -0.43295327 -0.6461004   0.738849    0.0328218   0.0188416\n",
      "  0.8649097  -0.41077766  0.17886354 -0.25267312 -0.5863404   0.3199377\n",
      "  0.06500584 -0.85005206  0.00927983 -0.53375626  0.24520755  0.4077925\n",
      "  0.3995235   0.17707741 -0.11359952 -0.05655043  0.8320879  -0.08225454\n",
      " -0.20435016  0.17938742  0.33883274  0.0179447   0.30879086  0.21830897\n",
      "  0.15148151  0.5075108  -0.48164928 -0.2729944  -0.27043518  0.35229757\n",
      " -0.13374902  0.02041686  0.07321578  0.0217213  -0.3316865  -0.11034877\n",
      " -0.15379266  0.05588889 -0.07496154  0.04026857  0.72076666  0.03823589\n",
      "  0.3727683  -0.02948365  0.24789341  0.19362195 -0.21636215 -0.30566362\n",
      "  0.06188507  0.3445202  -0.32349944 -0.3354765  -0.03056446 -0.15853581\n",
      " -0.15508455  0.54865414  0.04075445  0.6529362   0.47222438  0.29660028\n",
      " -0.3619578   0.27155215 -0.49596    -0.1248081  -0.01803662 -0.5289812\n",
      "  0.53570545  0.2536115  -0.17436077  0.23032275  0.6016395  -0.5348849\n",
      " -0.34507757  0.21349393 -0.44534764 -0.14593755 -0.5423269  -0.58697075\n",
      " -0.09033801  0.03374491 -0.27154562 -0.48369735  0.31984028  0.20112371\n",
      " -0.06003525 -0.1049942  -0.25834998 -0.42149878 -0.22873524 -0.2808335\n",
      " -0.5724169   0.21879491 -0.19751823 -0.4640846  -0.03336182 -0.5621598\n",
      "  0.23687465 -0.32349586 -0.5806219   0.22739647 -0.07238264 -0.5016004\n",
      " -0.69137114 -0.13462846 -0.6086641  -0.2414331   0.27815777 -0.20079619\n",
      "  0.2636771  -0.22463737 -0.24449675  0.18761759  0.24441864 -0.90628904\n",
      " -0.23366846 -0.70606077 -0.5029385   0.11913604 -0.00378895  0.01211156\n",
      " -0.3091525   0.12456799 -0.04546164 -0.3364252  -0.30129543  0.09922665\n",
      " -0.7171528   0.01411676  0.6556436  -0.22801217 -1.2658143   0.19369322\n",
      "  0.65262544  0.20712909  0.02525535 -0.04521104 -0.07721866 -0.3905838\n",
      " -0.37324962 -0.37386495 -0.47021237  0.21658854 -0.17566001 -0.7852305\n",
      "  0.12317482  0.3914031  -0.48560265 -0.13003549  0.27370977  0.04994999\n",
      " -0.1909389   0.01681341  0.32784778 -0.41714355  0.72821367  0.10795394\n",
      " -0.9120635  -0.2901761  -0.12562332  0.8948511  -0.68115133  0.19818059\n",
      " -0.07247967  0.07112893  0.0273707   0.6415572   0.5771735   0.14220408\n",
      "  0.0618895  -0.23894519 -0.2185234  -0.5809682   0.24778035 -0.52636707\n",
      "  0.13104974  0.25968683  0.42409587 -0.5996987  -0.19583377 -0.15495616\n",
      " -0.29773244 -0.10179307  0.2705412  -0.45278633 -0.81799364  0.41292757\n",
      "  0.03214647  0.9109482  -0.23420519  0.3300642   0.0609505   0.12148148\n",
      "  0.28252348  0.22165073  0.35740146 -0.6132218   0.24129027 -0.21292035]\n"
     ]
    }
   ],
   "source": [
    "# word2vec_model = Word2Vec.load(\"./word2vec/word2vec_model\")\n",
    "# word_vectors = word2vec_model.wv\n",
    "# print(word_vectors['human']) #looking good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#word2vec_model=Word2Vec.load(\"./word2vec/word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_length_dataframe(df):\n",
    "    # Create a new DataFrame with the required columns\n",
    "    length_df = pd.DataFrame({\n",
    "        'reference': df['reference'],\n",
    "        'len(text)': df['text'].apply(lambda x: len(str(x).split())),\n",
    "        'len(preprocessed_text)': df['preprocessed_text'].apply(lambda x: len(str(x).split()))\n",
    "    })\n",
    "    \n",
    "    return length_df\n",
    "\n",
    "df_length=create_length_dataframe(df_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size):\n",
    "    words = text.split()  # Split text into words\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        yield ' '.join(words[i:i + chunk_size])\n",
    "\n",
    "chunks_data = []\n",
    "\n",
    "# Iterate over the DataFrame and chunk the text\n",
    "for index, row in df_prep.iterrows():\n",
    "    text_chunks = list(chunk_text(row['preprocessed_text'], 5000))\n",
    "    for i, chunk in enumerate(text_chunks, 1):\n",
    "        chunks_data.append({\n",
    "            'preprocessed_text': chunk,\n",
    "            'reference': f\"{row['reference']}_{i}\"\n",
    "        })\n",
    "\n",
    "# Create the new DataFrame from the list of data\n",
    "df_chunk = pd.DataFrame(chunks_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_chunk.to_csv('./prepocessing/results/df_books_chunk.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
